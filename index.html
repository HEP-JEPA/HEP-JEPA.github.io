<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HEP-JEPA: A foundation model for collider physics using joint embedding
  predictive architecture">
  <meta name="keywords" content="JEPA, HEP-JEPA, P-JEPA, Particle-JEPA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HEP-JEPA: A foundation model for collider physics using joint embedding
  predictive architecture</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLF4JPQMF9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QLF4JPQMF9');
  </script></script></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Merriweather&family=Inter&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HEP-JEPA: A foundation model for collider physics using joint embedding
            predictive architecture</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jaibardhan.com/">Jai Bardhan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://in.linkedin.com/in/radhikeshagrawal">Radhikesh Agrawal*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://in.linkedin.com/in/abhiram-tilak-575402348">Abhiram Tilak*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://inspirehep.net/authors/1904817">Cyrin Neeraj</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/subhadipmitra/">Subhadip Mitra</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">International Institute of Information Technology, Hyderabad</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">*Equal Contributions</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.03933"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.03933"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-greyed-out is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code release soon!</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://zenodo.org/records/6619768"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr style="border: none; height: 2px; box-shadow: 0px 4px 6px -4px rgba(0, 0, 0, 0.5); margin: 20px 0;">


<section class="section">
  <div class="container is-max-desktop content box">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a transformer architecture-based foundation model 
            for tasks at high-energy particle colliders such as the Large 
            Hadron Collider. We train the model to classify jets using a self-supervised
            strategy inspired by the <a href="https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/">Joint Embedding Predictive Architecture </a>. We use the <a href="https://zenodo.org/records/6619768">JetClass dataset</a> containing
            100M jets of various known particles to pre-train the model with a data-centric approach â€” the
            model uses a fraction of the jet constituents as the context to predict the embeddings of the unseen
            target constituents. Our pre-trained model fares well with other datasets for standard classification 
            benchmark tasks. We test our model on two additional downstream tasks: top tagging and differentiating 
            light-quark jets from gluon jets. We also evaluate our model with task-specific metrics and baselines 
            and compare it with state-of-the-art models in high-energy physics.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container-flex">
      <div class="image-container">
        <br>
        <br>
        <img src="./static/images/jepa_schematic_combined.svg"
             class="interpolation-image"
             alt="Schematic Diagram for HEP-JEPA"/>
        <br>
        <br>
      </div>
      <div class="text-container">
        <h2 class="title is-3">HEP-JEPA Framework</h2>
        <p>

          Schematic diagram illustrating the working of the HEP-JEPA model. The model has a structure similar to vision transformers.
          In the first step, the entire jet is divided into patches using a particle jet tokeniser. These tokens are then masked to form the context and
          target blocks. Each block is fed into the respective encoder to generate the embeddings. The context embedding, along with the special
          mask tokens, is used by the predictor to predict the embedding of the masked target blocks.
</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div 
      class="columns is-centered content box"
      style="border-radius: 12px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);"
    >
      <!-- Experiments -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Experiments</h2>
            <h3>Few-Shot Learning Evaluations</h3>
            <p>The model was evaluated on the <strong>JetClass</strong> dataset, where it consistently outperformed models trained from scratch, particularly in low-label regimes:</p>
              <ul>
                <li>Two regimes, frozen (pretrained backbone not updated) and fine-tuned were evaluated</li>
                <li>Evaluated at label fractions: 0.05%, 0.5%, 2%, 10%, and 100%</li>
                <li>Compared pre-trained HEP-JEPA model with model trained from scratch</li>
              </ul>

            <h3>Downstream Task Evaluations</h3>
            <p>The model was tested on two critical tasks:</p>
            <ul>
                <li><strong>Top tagging</strong> using the <em>Top Tagging Reference</em> dataset</li>
                <li><strong>Quark-gluon jet differentiation</strong> using the <em>quark-gluon tagging</em> dataset</li>
            </ul>

            <h3>Ablation Studies</h3>
            <p>The study explored various design choices, including:</p>
            <ul>
                <li>Masking strategies (<strong>random vs. contiguous</strong> token selection)</li>
                <li>Number of target tokens to predict</li>
                <li>Physics bias in the attention mechanism</li>
                <li>Integration of <strong>register tokens</strong></li>
                <li>Impact of <strong>physics-inspired data augmentations</strong></li>
            </ul>

        </div>
      </div>

      <!-- Results. -->
      <div class="column">
        <h2 class="title is-3">Results and Findings</h2>
        <div class="columns is-centered">
          <div class="column content">
            <h3>Key Findings</h3>
            <ul>
                <li><span class="highlight">Physics bias</span> improved performance by approximately <strong>2%</strong></li>
                <li><span class="highlight">Register tokens</span> increased performance by around <strong>2%</strong></li>
                <li>The <strong>contiguous masking strategy</strong> with one target token performed best</li>
                <li>Physics-inspired augmentations did not significantly improve performance</li>
            </ul>

            <h3>JetClass Metrics</h3>
            <table>
                <thead>
                    <tr>
                        <th>% of Labels (Size)</th>
                        <th>Model</th>
                        <th>Accuracy</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="2">0.05% (5K)</td>
                        <td>From Scratch</td>
                        <td>0.505</td>
                    </tr>
                    <tr>
                        <td>HEP-JEPA, Fine-Tuning</td>
                        <td>0.564</td>
                    </tr>
                    <tr>
                        <td rowspan="2">0.5% (50K)</td>
                        <td>From Scratch</td>
                        <td>0.586</td>
                    </tr>
                    <tr>
                        <td>HEP-JEPA, Fine-Tuning</td>
                        <td>0.624</td>
                    </tr>
                    <tr>
                        <td rowspan="2">2% (2M)</td>
                        <td>From Scratch</td>
                        <td>0.668</td>
                    </tr>
                    <tr>
                        <td>HEP-JEPA, Fine-Tuning</td>
                        <td>0.669</td>
                    </tr>
                    <tr>
                        <td rowspan="2">10% (10M)</td>
                        <td>From Scratch</td>
                        <td>0.683</td>
                    </tr>
                    <tr>
                        <td>HEP-JEPA, Fine-Tuning</td>
                        <td>0.685</td>
                    </tr>
                    <tr>
                        <td rowspan="2">100% (100M)</td>
                        <td>From Scratch</td>
                        <td>0.698</td>
                    </tr>
                    <tr>
                        <td>HEP-JEPA, Fine-Tuning</td>
                        <td>0.698</td>
                    </tr>
                </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>

    <hr style="border: none; height: 2px; box-shadow: 0px 4px 6px -4px rgba(0, 0, 0, 0.5); margin: 20px 0;">


    <div class="columns is-centered">
      <div class="column">
        <div 
          class="content box has-background-white-ter p-5"
          style="border-radius: 12px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);"
        >
          <h2 class="title is-3">Validation Loss Performance</h2>
          <p>
            Validation loss vs. training step for the two benchmark
            odels training in a few-shot learning setting for jet classification
            n the JetClass dataset with 0.5% labels (i.e., 50000 training sam-
            les). One model is trained from scratch, whereas the pre-trained
            EP-JEPA model is fine-tuned.
          </p>
          <div class="is-flex is-justify-content-center">
            <img 
              src="./static/images/val_50k.svg" 
              alt="Validation loss vs. Training step"
              style="width: 80%; padding-right: 40px"
            />
          </div>
          <p style="margin-top: 20px">
            The validation loss falls quickly
            or the HEP-JEPA model â€” it achieves the same minimum valida-
            ion loss as the model trained from scratch three times faster
          </p>
        </div>
      </div>

      <!-- Results. -->
      <div class="column">
        <div 
          class="content box has-background-white-ter p-5"
          style="border-radius: 12px; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);"
        >
          <h2 class="title is-3">Visualization</h2>
          <p>
            We visualise the representation learned by HEP-JEPA on
            50k samples of JetClass sampled uniformly from each class.
            We construct the embedding for a sample by concatenating
            the max and mean pooling of the outputs of the context
            encoder and apply t-SNE on the pooled embedding.
          </p>
          <div class="is-flex is-justify-content-center">
            <img 
              src="./static/images/tsne.png" 
              alt="Validation loss vs. Training step"
              style="width: 80%; border-radius: 8px;"
            />
          </div>
          <p>
            We observe that events that
            contain lepton(s) are pushed to the right, while hadronic
            events are more towards the left.
          </p>


        </div>
      </div>
    </div>


    <hr style="border: none; height: 2px; box-shadow: 0px 4px 6px -4px rgba(0, 0, 0, 0.5); margin: 20px 0;">


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>Several recent works have explored foundation models and self-supervised learning in high-energy physics (HEP).</p>  

          <p><a href="https://arxiv.org/abs/2404.16091">OmniLearn</a> and <a href="https://arxiv.org/abs/2202.03772">Particle Transformer</a> use transformer-based architectures for HEP tasks, relying on supervised learning with simulated data and generative modelling.</p>  

          <p><a href="https://arxiv.org/abs/2401.13537">Masked Particle Modelling (MPM)</a> and <a href="https://arxiv.org/abs/2403.05618">OmniJet-Î±</a> adapt masked modeling and generative pre-training from natural language processing to collider physics.</p>  

          <p>Concurrent to our work, <a href="https://arxiv.org/abs/2412.05333">J-JEPA</a> adapts the JEPA paradigm for the task of top tagging â€” the authors pre-train the model on 1% of the top jet and light jet samples from JetClass and evaluate downstream performance on the Top Tagging Reference dataset. However, unlike our data-centric approach, the authors generate context / target tokens through clustering subjets. We also show more comprehensive evaluations on the entire JetClass dataset and downstream applications with better performance on the tasks.</p>

          <p>Contrastive learning methods, such as those in <a href="https://arxiv.org/abs/2108.04253">Dillon et al. (2022)</a>, follow frameworks like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, but require carefully selected negative samples. In contrast, <a href="https://arxiv.org/abs/2301.08243">Joint Embedding Predictive Architectures (JEPA)</a> have shown promising results in images, videos, and point clouds by learning in latent space without a decoder.</p>  

          <p>For an extensive survey on foundation models in HEP, see <a href="https://indico.in2p3.fr/event/33412/contributions/143801/attachments/87125/131479/Foundation_models_for_HEP_Anna_Hallin.pdf">this</a> and <a href="https://iml-wg.github.io/HEPML-LivingReview/#foundation-models-llms">this</a>.</p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<hr style="border: none; height: 2px; box-shadow: 0px 4px 6px -4px rgba(0, 0, 0, 0.5); margin: 20px 0;">

<section class="section" id="Sponsors">
  <div class="container is-max-desktop content">
    <h2 class="title">Sponsors</h2>
    <div class="content has-text-justified">
      <p>We are looking for sponsors and collaborators! </p>
      <p> If you would like to sponsor or collaborate on future iterations of this work (or other works from our lab) please reach out to subhadip.mitra&lt;AT&gt;iiit.ac.in or jai.bardhan&lt;AT&gt;alumni.iiit.ac.in</p>
    </div>
  </div>
</section>

<hr style="border: none; height: 2px; box-shadow: 0px 4px 6px -4px rgba(0, 0, 0, 0.5); margin: 20px 0;">

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{bardhan2025hepjepafoundationmodelcollider,
      title={HEP-JEPA: A foundation model for collider physics using joint embedding predictive architecture}, 
      author={Jai Bardhan and Radhikesh Agrawal and Abhiram Tilak and Cyrin Neeraj and Subhadip Mitra},
      year={2025},
      eprint={2502.03933},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.03933}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  
  <div class="container">
    <!-- <div class="content has-text-centered"> -->
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    <!-- </div> -->
    <div class="columns is-centered">
      <div class="column is-6 is-size-6">
        <div class="content">
          <p>
            This website is based on the template from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>

</footer>

</body>
</html>
