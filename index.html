<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HEP-JEPA: A foundation model for collider physics using joint embedding
  predictive architecture">
  <meta name="keywords" content="JEPA, HEP-JEPA, P-JEPA, Particle-JEPA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HEP-JEPA: A foundation model for collider physics using joint embedding
  predictive architecture</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLF4JPQMF9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QLF4JPQMF9');
  </script></script></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HEP-JEPA: A foundation model for collider physics using joint embedding
            predictive architecture</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://in.linkedin.com/in/jai-bardhan">Jai Bardhan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://in.linkedin.com/in/radhikeshagrawal">Radhikesh Agrawal</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://in.linkedin.com/in/abhiram-tilak-575402348">Abhiram Tilak</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://inspirehep.net/authors/1904817">Cyrin Neeraj</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/subhadipmitra/">Subhadip Mitra</a><sup>3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">International Instituite of Information Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-greyed-out is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-greyed-out is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-greyed-out is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://zenodo.org/records/6619768"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a transformer architecture-based foundation model 
            for tasks at high-energy particle colliders such as the Large 
            Hadron Collider. We train the model to classify jets using a self-supervised
            strategy inspired by the <a href="https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/">Joint Embedding Predictive Architecture </a>.
          </p>

          <p>
          We use the <a href="https://zenodo.org/records/6619768">JetClass dataset</a> containing
          100M jets of various known particles to pre-train the model with a data-centric approach — the
          model uses a fraction of the jet constituents as the context to predict the embeddings of the unseen
          target constituents. Our pre-trained model fares well with other datasets for standard classification 
          benchmark tasks. We test our model on two additional downstream tasks: top tagging and differentiating 
          light-quark jets from gluon jets. We also evaluate our model with task-specific metrics and baselines 
          and compare it with state-of-the-art models in high-energy physics.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container-flex">
      <div class="image-container">
        <br>
        <br>
        <img src="./static/images/jepa_schematic_combined.svg"
             class="interpolation-image"
             alt="Interpolate start reference image."/>
        <br>
        <br>
      </div>
      <div class="text-container">
        <h2 class="title is-3">HEP-JEPA Framework</h2>
        <p>

          Schematic diagram illustrating the working of the HEP-JEPA model. The model has a structure similar to vision transformers.
          In the first step, the entire jet is divided into patches using a particle jet tokeniser. These tokens are then masked to form the context and
          target blocks. Each block is fed into the respective encoder to generate the embeddings. The context embedding, along with the special
          mask tokens, is used by the predictor to predict the embedding of the masked target blocks.
</p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>Several recent works have explored foundation models and self-supervised learning in high-energy physics (HEP).</p>  

          <p><a href="https://arxiv.org/abs/2404.16091">OmniLearn</a> and <a href="https://arxiv.org/abs/2202.03772">Particle Transformer</a> use transformer-based architectures for HEP tasks, relying on supervised learning with simulated data.</p>  

          <p><a href="https://arxiv.org/abs/2401.13537">Masked Particle Modelling (MPM)</a> and <a href="https://arxiv.org/abs/2403.05618">OmniJet-α</a> adapt masked modeling and generative pre-training from natural language processing to collider physics.</p>  

          <p>Contrastive learning methods, such as those in <a href="https://arxiv.org/abs/2108.04253">Dillon et al. (2022)</a>, follow frameworks like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, but require carefully selected negative samples. In contrast, <a href="https://arxiv.org/abs/2301.08243">Joint Embedding Predictive Architectures (JEPA)</a> have shown promising results in images, videos, and point clouds by learning in latent space without a decoder.</p>  

          <p>For an extensive survey on foundation models in HEP, see <a href="https://indico.in2p3.fr/event/33412/contributions/143801/attachments/87125/131479/Foundation_models_for_HEP_Anna_Hallin.pdf">this</a>.</p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>article{bardhan2025hepjepa,
  author    = {Bardhan, Jai and Agrawal, Radhikesh and Tilak, Abhiram and Neeraj, Cyrin and Mitra, Subhadip},
  title     = {HEP-JEPA: A foundation model for collider physics using joint embedding predictive architecture},
  journal   = {TBD},
  year      = {2025},
  institution = {International Institute of Information Technology}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
